{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training a Smart Cab**\n",
    "#**Implement a Basic Driving Agent**\n",
    "#**After Random Action implementation**\n",
    "QUESTION: Observe what you see with the agent's behavior as it takes random actions. Does the smart cab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\t\t\t\t\t\t\n",
    "There is only a change of code for action to choose an action randomly through the list valid_actions = [None, 'forward', 'left', 'right']. \n",
    " The cab initially moves about at random. Sometimes it happens upon a waypoint, and with sufficient running time and no obstacles it will always eventually hit the waypoint given infinite time, but there may be no guarantee that the agent will do the same given finite time no matter how large the given time constraint may be.\t\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**After State Storage**\n",
    "\t\t\t\t\t\t\n",
    "QUESTION: What states have you identified that are appropriate for modeling the smart cab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\t\t\t\t\t\t\n",
    "The parameters that  go in local state for the agent should be bits of data that are useful in deciding the next best course of action. In addition to the desired direction of travel from the planner, almost every input qualifies with the exception of what any car to the right in the current intersection is planning to do.\n",
    "\t\t\t\t\t\t\n",
    "We require the direction of travel (next_waypoint) because the direction of the next waypoint tells the agent which way we would generally prefer to travel; without this information we wouldn't have a reason to turn left (for instance) instead of going straight, or any reason to travel any particular direction really.\n",
    "\t\t\t\t\t\t\n",
    "We  need to know whether the light is green or red because that limits whether we can take our desired action right now (we'd prefer to travel forward, for example, but if we do so when the light is red that's a traffic infraction with negative reward).\n",
    "\t\t\t\t\t\t\n",
    "We have to know the status of any cars at the intersection oncoming or to the left, because they can interfere with a desired action. If we want to travel right, we can do so on a red light as long as there are no cars from the left traveling through. If we want to travel left we can do so as long as there is no oncoming car traveling straight across.\n",
    "\t\t\t\t\t\t\n",
    "I can't think of a scenario in which we care about cars to the right, though. If we want to go straight, then if the light is green we can go, and if red we can't. If we want to go right, then on green we just go, on red we care what the car from the left wants to do. If we want to go left, then on red we stop, and on green we care what the oncoming car wants to do. In no case do we care about the intentions of the car to the right (unless that car is not a reliable rule follower, something we might actually want to consider in a more realistic simulation).\n",
    "\t\t\t\t\t\t\n",
    "We also don't really care about deadline, since regardless of how long is left on the clock we still want to take the optimal action at each step (we would not want to make a car that would break safety laws when in a hurry, this wouldn't be a lot better than a human driver). Additionally, there are a lot of possible deadline values, which would explode the number of states we need to account for in the learning matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        self.state = {}\n",
    "        self.learning_rate = 0.6\n",
    "        self.exploration_rate = 0.1\n",
    "        self.exploration_degradation_rate = 0.001\n",
    "        self.discount_rate = 0.4\n",
    "        self.q_values = {}\n",
    "        self.valid_actions = [None, 'forward', 'left', 'right']\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        self.state = self.build_state(inputs)\n",
    "\n",
    "        # TODO: Select action according to your policy\n",
    "        action = self.choose_action_from_policy(self.state)\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "        self.update_q_value(self.state, action, reward)\n",
    "\n",
    "        #print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(deadline, inputs, action, reward)  # [debug]\n",
    "\n",
    "    def build_state(self, inputs):\n",
    "      return {\n",
    "        \"light\": inputs[\"light\"],\n",
    "        \"oncoming\": inputs[\"oncoming\"],\n",
    "        \"left\": inputs[\"left\"],\n",
    "        \"direction\": self.next_waypoint\n",
    "      }\n",
    "\n",
    "    def choose_action_from_policy(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            self.exploration_rate -= self.exploration_degradation_rate\n",
    "            return random.choice(self.valid_actions)\n",
    "        best_action = self.valid_actions[0]\n",
    "        best_value = 0\n",
    "        for action in self.valid_actions:\n",
    "            cur_value = self.q_value_for(state, action)\n",
    "            if cur_value > best_value:\n",
    "                best_action = action\n",
    "                best_value = cur_value\n",
    "            elif cur_value == best_value:\n",
    "                best_action = random.choice([best_action, action])\n",
    "        return best_action\n",
    "\n",
    "    def max_q_value(self, state):\n",
    "        max_value = None\n",
    "        for action in self.valid_actions:\n",
    "            cur_value = self.q_value_for(state, action)\n",
    "            if max_value is None or cur_value > max_value:\n",
    "                max_value = cur_value\n",
    "        return max_value\n",
    "\n",
    "    def q_value_for(self, state, action):\n",
    "        q_key = self.q_key_for(state, action)\n",
    "        if q_key in self.q_values:\n",
    "            return self.q_values[q_key]\n",
    "        return 0\n",
    "\n",
    "    def update_q_value(self, state, action, reward):\n",
    "        q_key = self.q_key_for(state, action)\n",
    "        cur_value = self.q_value_for(state, action)\n",
    "        inputs = self.env.sense(self)\n",
    "        self.next_waypoint = self.planner.next_waypoint()\n",
    "        new_state = self.build_state(inputs)\n",
    "        learned_value = reward + (self.discount_rate * self.max_q_value(new_state))\n",
    "        new_q_value = cur_value + (self.learning_rate * (learned_value - cur_value))\n",
    "        self.q_values[q_key] = new_q_value\n",
    "\n",
    "    def q_key_for(self, state, action):\n",
    "        return \"{}|{}|{}|{}|{}\".format(state[\"light\"], state[\"direction\"], state[\"oncoming\"], state[\"left\"], action)\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "\n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(LearningAgent)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=True)  # specify agent to track\n",
    "    # NOTE: You can set enforce_deadline=False while debugging to allow longer trials\n",
    "\n",
    "    # Now simulate it\n",
    "    sim = Simulator(e, update_delay=0.0, display=True)  # create simulator (uses pygame when display=True, if available)\n",
    "    # NOTE: To speed up simulation, reduce update_delay and/or set display=False\n",
    "\n",
    "    sim.run(n_trials=100)  # run for a specified number of trials\n",
    "    # NOTE: To quit midway, press Esc or close pygame window, or hit Ctrl+C on the command-line\n",
    "    print \"CONCLUSION REPORT\"\n",
    "    print \"WINS: {}\".format(e.wins)\n",
    "    print \"LOSSES: {}\".format(e.losses)\n",
    "    print \"INFRACTIONS: {}\".format(e.infractions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
